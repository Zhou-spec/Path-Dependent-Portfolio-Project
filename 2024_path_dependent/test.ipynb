{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one path for n assets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from simulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size=1):  # Output size is set to 1 by default\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # LSTM layer expects input of shape (batch_size, seq_length, features)\n",
    "        # For a univariate sequence, features=1\n",
    "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "        # Fully connected layer to map the hidden state output to the desired output size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a batch dimension and feature dimension to x\n",
    "        # Reshaping x from (n,) to (1, n, 1) to fit LSTM input requirements\n",
    "        x = x.unsqueeze(0).unsqueeze(-1)  # Now x is of shape [1, seq_length, 1]\n",
    "        # Process x through the LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only use the output from the last time step\n",
    "        # This assumes you're interested in the final output for sequence processing\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        # Squeeze the output to remove 1-dimensions, aiming for a scalar output\n",
    "        return out.squeeze()\n",
    "\n",
    "# Initialize the model\n",
    "hidden_size = 10  # Number of LSTM units in the hidden layer\n",
    "\n",
    "model = SimpleLSTMModel(hidden_size)\n",
    "x = torch.randn(50)\n",
    "f = Functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59746057\n",
      "-0.019420683\n",
      "0.002566311\n"
     ]
    }
   ],
   "source": [
    "print(f.partial_t(x, 0.01))\n",
    "print(f.partial_x(x, 0.02))\n",
    "print(f.partial_xx(x, 0.03))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = generate_asset_path(mu = np.array([0.1, 0.2]), sigma = np.array([[0.3, 0.4], [0.1, 0.2]]), T = 1, dt = 0.01)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48283133709678583"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a class of trading policy \n",
    "# the trading policy object\n",
    "\n",
    "naive_states = np.array([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6]])\n",
    "\n",
    "policy = Trading_Policy(f, naive_states)\n",
    "\n",
    "policy.entropy(path[0], mu = np.array([0.1, 0.2]), sigma = np.array([[0.3, 0.4], [0.1, 0.2]]), dt = 0.01, h = 0.02, gamma = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_history, wealth_history = simulated_trading(policy, path, 1, np.array([0.1, 0.2]), np.array([[0.3, 0.4], [0.1, 0.2]]), 0.01, 0.1, 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.00255504 1.00065949 1.00309649 0.98839989 0.97981859\n",
      " 1.00126334 1.00387321 1.0908802  1.09243676 1.078931   1.06882925\n",
      " 1.07361598 1.01339509 1.02229497 1.03199591 1.04085764 1.07262394\n",
      " 1.06402497 1.00691036 1.02431476 1.00941354 1.01356804 1.03489285\n",
      " 1.03074069 1.06572    1.15833184 1.13606404 1.12131189 1.10846168\n",
      " 1.09899142 1.08988344 1.11094287 1.10851342 1.11657737 1.11457145\n",
      " 1.12916296 1.12714306 1.14734393 1.13090829 1.1226665  1.16761094\n",
      " 1.21071511 1.22798497 1.23577677 1.27011818 1.28298025 1.2642871\n",
      " 1.26731789 1.27280582 1.28110765 1.28984079 1.27357197 1.28666954\n",
      " 1.26778151 1.26251127 1.24865546 1.21684725 1.24015009 1.25679043\n",
      " 1.22907078 1.22341448 1.25622898 1.24364611 1.26368405 1.31406809\n",
      " 1.2708017  1.26389787 1.25631698 1.27768157 1.25272251 1.25424772\n",
      " 1.30605481 1.30538904 1.26991672 1.22431906 1.24587578 1.23835156\n",
      " 1.25213454 1.22280639 1.21005233 1.20900439 1.19518256 1.1988265\n",
      " 1.20040338 1.19692251 1.18919077 1.18818415 1.24560712 1.25692818\n",
      " 1.24002994 1.26399977 1.27265787 1.27030767 1.25854487 1.258664\n",
      " 1.23079038 1.24471887 1.24275578 1.24374974]\n"
     ]
    }
   ],
   "source": [
    "print(wealth_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10818638840399397"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement the loss function\n",
    "\n",
    "gamma = 0.3\n",
    "mu = np.array([0.1, 0.2])\n",
    "sigma = np.array([[0.3, 0.4], [0.1, 0.2]])\n",
    "\n",
    "# the following function is to calculate the loss of one path\n",
    "# using the continuous TD error\n",
    "def one_path_loss(new_value_net, policy, wealth_history, dt, h, gamma, mu, sigma):\n",
    "    # wealth_history: path of wealth\n",
    "    # policy: trading policy object\n",
    "    # new_value_net: new value network, the network need to be trained \n",
    "\n",
    "    n = len(wealth_history)\n",
    "    TD_error = np.zeros(n)\n",
    "\n",
    "    # make the wealth_hisotry as 1 dimensional tensor \n",
    "    wealth_history = torch.tensor(wealth_history, dtype = torch.float32)\n",
    "\n",
    "    for i in range(2, n):\n",
    "        x = wealth_history[:i]\n",
    "        x_short = wealth_history[:i-1]\n",
    "        value_derivative = (new_value_net(x) - new_value_net(x_short)) / dt\n",
    "        entropy = policy.entropy(x, mu, sigma, dt, h, gamma)\n",
    "        TD_error[i] = value_derivative + entropy\n",
    "\n",
    "    # return the sum of square of TD_error\n",
    "    return 0.5 * np.sum(TD_error**2) * dt\n",
    "\n",
    "\n",
    "new_value_net = SimpleLSTMModel(24)\n",
    "\n",
    "wealth_hisotry = torch.tensor(wealth_history, dtype = torch.float32)\n",
    "\n",
    "one_path_loss(new_value_net, policy, wealth_history, 0.01, 0.02, 0.3, np.array([0.1, 0.2]), np.array([[0.3, 0.4], [0.1, 0.2]]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qids-2023-comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
